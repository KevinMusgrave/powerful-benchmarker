{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Powerful Benchmarker \u00b6 Installation \u00b6 pip install powerful-benchmarker Modules that can be benchmarked \u00b6 By default, you can access the following modules via the command line and the yaml config files: pytorch-metric-learning torch.nn torch.optim torch.optim.lr_scheduler torch.utils.data torchvision.transforms.transforms torchvision.datasets torchvision.models pretrainedmodels You can add other classes and modules by using the register functionality . Getting started \u00b6 Set default flags \u00b6 The easiest way to get started is to download the example script . Then change the default values for the following flags: pytorch_home is where you want to save downloaded pretrained models. dataset_root is where your datasets will be downloaded, or where they are already located. root_experiment_folder is where you want all experiment data to be saved. Try a basic command \u00b6 The following command will run an experiment using the default config files , as well as download the CUB200 dataset into your dataset_root python run.py --experiment_name test1 --dataset {CUB200: {download: True}} If the code runs properly, you'll see training and testing progress like this: Experiment folder format \u00b6 Experiment data is saved in the following format: <root_experiment_folder> |-<experiment_name> |-configs |-<split scheme name> |-saved_models |-saved_csvs |-tensorboard_logs |-meta_logs |-saved_csvs |-tensorboard_logs Here's what's in each subfolder: configs contains the yaml config files necessary to reproduce the experiment. <split scheme name>/save_models contains saved pytorch models for a particular split scheme. (A split scheme simply refers to the way train/val/test splits are formed.) <split scheme name>/saved_csvs contains CSV files with data collected during training. It also contains an SQLite database file with the same data. <split scheme name>/tensorboard_logs contains the same information in <split scheme name>/saved_csvs , but in tensorboard format. meta_logs/saved_csvs contains CSV files for aggregate and ensemble accuracies. It also contains an SQLite database file with the same data. meta_logs/tensorboard contains the same information in meta_logs/save_csvs , but in tensorboard format. View experiment data \u00b6 There are multiple ways to view experiment data: Tensorboard \u00b6 Go to the <experiment_name> folder, and run tensorboard at the command line: tensorboard --logdir = . --port = 12345 Then in your web browser, go to localhost:<port> , where <port> is specified in the tensorboard command. You'll see plots like this: CSV \u00b6 Use any text editor or spreadsheet program to view the csv files that are saved in the saved_csvs folders. SQLite \u00b6 Use DB Browser to open the database files that are saved in the saved_csvs folders. Resume training \u00b6 You can interrupt the program and resume training at a later time: python run.py --experiment_name test1 --resume_training latest You can also resume using the model with the best validation accuracy: python run.py --experiment_name test1 --resume_training best Keep track of changes \u00b6 Let's say you finished training for 100 epochs, and decide you want to train for another 50 epochs, for a total of 150. You would run: python run.py --experiment_name test1 --resume_training latest \\ --num_epochs_train 150 --merge_argparse_when_resuming (The merge_argparse_when_resuming flag tells the code that you want to make changes to the original experiment configuration. If you don't use this flag, then the code will ignore your command line arguments, and use the original configuration. The purpose of this is to avoid accidentally changing configs in the middle of an experiment.) Now in your experiments folder you'll see the original config files, and a new folder starting with resume_training . <root_experiment_folder> |-<experiment_name> |-configs |-resume_training_config_diffs_<underscore delimited numbers> ... This folder contains all differences between the originally saved config files and the parameters that you've specified at the command line. In this particular case, there should just be a single file config_general.yaml with a single line: num_epochs_train: 150 . The underscore delimited numbers in the folder name indicate which models were loaded for each split scheme . For example, let's say you are doing cross validation with 3 folds. The training process has finished 50, 30, and 0 epochs of folds 0, 1, and 2, respectively. You decide to stop training, and resume training with a different batch size. Now the config diff folder will be named resume_training_config_diffs_50_30_0 . Reproduce an experiment \u00b6 To reproduce an experiment, use the --reproduce_results flag, and pass in the path to the experiment folder you want to reproduce: python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> This will run an experiment based on the config files in experiment_to_reproduce . You can make modifications to the configuration at the command line, as long as you provide the --merge_argparse_when_resuming flag, so that the code knows you intend on making changes: # reproduce the experiment but use a different number of dataloaders python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> \\ --trainer~APPLY~2: { dataloader_num_workers: 16 } \\ --tester~APPLY~2: { dataloader_num_workers: 16 } \\ --merge_argparse_when_resuming For a guide on how to reproduce the results of A Metric Learning Reality Check , see the supplementary material Evaluating on specific splits \u00b6 By default, your model will be saved and evaluated on the validation set every save_interval epochs. To get accuracy for specific splits, use the --splits_to_eval flag and pass in a python-style list of split names: --splits_to_eval [train, test] . To run evaluation only, use the --evaluate or --evaluate_ensemble flag. Cross validation split schemes \u00b6 In this library, splits are not hard-coded into the dataset classes. Instead, train/val/test splits are created by a SplitManager , as specified in the config_dataset file: split_manager : ClassDisjointSplitManager : test_size : 0.5 test_start_idx : 0.5 num_training_partitions : 4 num_training_sets : 4 This particular configuration will set aside the second 50% of classes for the test set. Then the first 50% of classes will be used for 4-fold cross validation, in which the train and val splits are always class-disjoint. Advanced usage \u00b6 Here are some other important features of this library: The powerful command line syntax that allows you to easily override, modify, merge, and delete config options at the command line, and within yaml files. Easy and flexible hyperparameter optimization The ability to add custom modules , without having to delve into the benchmarking code.","title":"Home"},{"location":"#powerful-benchmarker","text":"","title":"Powerful Benchmarker"},{"location":"#installation","text":"pip install powerful-benchmarker","title":"Installation"},{"location":"#modules-that-can-be-benchmarked","text":"By default, you can access the following modules via the command line and the yaml config files: pytorch-metric-learning torch.nn torch.optim torch.optim.lr_scheduler torch.utils.data torchvision.transforms.transforms torchvision.datasets torchvision.models pretrainedmodels You can add other classes and modules by using the register functionality .","title":"Modules that can be benchmarked"},{"location":"#getting-started","text":"","title":"Getting started"},{"location":"#set-default-flags","text":"The easiest way to get started is to download the example script . Then change the default values for the following flags: pytorch_home is where you want to save downloaded pretrained models. dataset_root is where your datasets will be downloaded, or where they are already located. root_experiment_folder is where you want all experiment data to be saved.","title":"Set default flags"},{"location":"#try-a-basic-command","text":"The following command will run an experiment using the default config files , as well as download the CUB200 dataset into your dataset_root python run.py --experiment_name test1 --dataset {CUB200: {download: True}} If the code runs properly, you'll see training and testing progress like this:","title":"Try a basic command"},{"location":"#experiment-folder-format","text":"Experiment data is saved in the following format: <root_experiment_folder> |-<experiment_name> |-configs |-<split scheme name> |-saved_models |-saved_csvs |-tensorboard_logs |-meta_logs |-saved_csvs |-tensorboard_logs Here's what's in each subfolder: configs contains the yaml config files necessary to reproduce the experiment. <split scheme name>/save_models contains saved pytorch models for a particular split scheme. (A split scheme simply refers to the way train/val/test splits are formed.) <split scheme name>/saved_csvs contains CSV files with data collected during training. It also contains an SQLite database file with the same data. <split scheme name>/tensorboard_logs contains the same information in <split scheme name>/saved_csvs , but in tensorboard format. meta_logs/saved_csvs contains CSV files for aggregate and ensemble accuracies. It also contains an SQLite database file with the same data. meta_logs/tensorboard contains the same information in meta_logs/save_csvs , but in tensorboard format.","title":"Experiment folder format"},{"location":"#view-experiment-data","text":"There are multiple ways to view experiment data:","title":"View experiment data"},{"location":"#tensorboard","text":"Go to the <experiment_name> folder, and run tensorboard at the command line: tensorboard --logdir = . --port = 12345 Then in your web browser, go to localhost:<port> , where <port> is specified in the tensorboard command. You'll see plots like this:","title":"Tensorboard"},{"location":"#csv","text":"Use any text editor or spreadsheet program to view the csv files that are saved in the saved_csvs folders.","title":"CSV"},{"location":"#sqlite","text":"Use DB Browser to open the database files that are saved in the saved_csvs folders.","title":"SQLite"},{"location":"#resume-training","text":"You can interrupt the program and resume training at a later time: python run.py --experiment_name test1 --resume_training latest You can also resume using the model with the best validation accuracy: python run.py --experiment_name test1 --resume_training best","title":"Resume training"},{"location":"#keep-track-of-changes","text":"Let's say you finished training for 100 epochs, and decide you want to train for another 50 epochs, for a total of 150. You would run: python run.py --experiment_name test1 --resume_training latest \\ --num_epochs_train 150 --merge_argparse_when_resuming (The merge_argparse_when_resuming flag tells the code that you want to make changes to the original experiment configuration. If you don't use this flag, then the code will ignore your command line arguments, and use the original configuration. The purpose of this is to avoid accidentally changing configs in the middle of an experiment.) Now in your experiments folder you'll see the original config files, and a new folder starting with resume_training . <root_experiment_folder> |-<experiment_name> |-configs |-resume_training_config_diffs_<underscore delimited numbers> ... This folder contains all differences between the originally saved config files and the parameters that you've specified at the command line. In this particular case, there should just be a single file config_general.yaml with a single line: num_epochs_train: 150 . The underscore delimited numbers in the folder name indicate which models were loaded for each split scheme . For example, let's say you are doing cross validation with 3 folds. The training process has finished 50, 30, and 0 epochs of folds 0, 1, and 2, respectively. You decide to stop training, and resume training with a different batch size. Now the config diff folder will be named resume_training_config_diffs_50_30_0 .","title":"Keep track of changes"},{"location":"#reproduce-an-experiment","text":"To reproduce an experiment, use the --reproduce_results flag, and pass in the path to the experiment folder you want to reproduce: python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> This will run an experiment based on the config files in experiment_to_reproduce . You can make modifications to the configuration at the command line, as long as you provide the --merge_argparse_when_resuming flag, so that the code knows you intend on making changes: # reproduce the experiment but use a different number of dataloaders python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> \\ --trainer~APPLY~2: { dataloader_num_workers: 16 } \\ --tester~APPLY~2: { dataloader_num_workers: 16 } \\ --merge_argparse_when_resuming For a guide on how to reproduce the results of A Metric Learning Reality Check , see the supplementary material","title":"Reproduce an experiment"},{"location":"#evaluating-on-specific-splits","text":"By default, your model will be saved and evaluated on the validation set every save_interval epochs. To get accuracy for specific splits, use the --splits_to_eval flag and pass in a python-style list of split names: --splits_to_eval [train, test] . To run evaluation only, use the --evaluate or --evaluate_ensemble flag.","title":"Evaluating on specific splits"},{"location":"#cross-validation-split-schemes","text":"In this library, splits are not hard-coded into the dataset classes. Instead, train/val/test splits are created by a SplitManager , as specified in the config_dataset file: split_manager : ClassDisjointSplitManager : test_size : 0.5 test_start_idx : 0.5 num_training_partitions : 4 num_training_sets : 4 This particular configuration will set aside the second 50% of classes for the test set. Then the first 50% of classes will be used for 4-fold cross validation, in which the train and val splits are always class-disjoint.","title":"Cross validation split schemes"},{"location":"#advanced-usage","text":"Here are some other important features of this library: The powerful command line syntax that allows you to easily override, modify, merge, and delete config options at the command line, and within yaml files. Easy and flexible hyperparameter optimization The ability to add custom modules , without having to delve into the benchmarking code.","title":"Advanced usage"},{"location":"cl_syntax/","text":"Command Line Syntax \u00b6 This library comes with a powerful command line syntax that makes it easy to change complex configuration options in a precise fashion. Lists and dictionaries \u00b6 Lists and dictionaries are written at the command line in python form: Example list: --splits_to_eval [ train, val, test ] Example nested dictionary --mining_funcs { tuple_miner: { MultiSimilarityMiner: { epsilon: 0 .1 }}} Merge \u00b6 Consider the following optimizer configuration. optimizers : trunk_optimizer : RMSprop : lr : 0.000001 At the command line, we can change lr to 0.01, and add alpha = 0.95 to the RMSprop parameters: --optimizers { trunk_optimizer: { RMSprop: { lr: 0 .01, alpha: 0 .95 }}} So in effect, the config file now looks like this: optimizers : trunk_optimizer : RMSprop : lr : 0.01 alpha : 0.95 In other words, we specify a dictionary at the command line, using python dictionary syntax. This dictionary is then merged into the one specified in the config file. Thus, adding keys is very straightforward: --optimizers { embedder_optimizer: { Adam: { lr: 0 .01 }}} Now the config file includes a specification for embedder_optimizer : optimizers : trunk_optimizer : RMSprop : lr : 0.000001 embedder_optimizer : Adam : lr : 0.01 But what happens if we try to set the trunk_optimizer to Adam ? --optimizers { trunk_optimizer: { Adam: { lr: 0 .01 }}} Now there's a problem with the config file, because two optimizer types are specified for a single optimizer: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 Adam : lr : 0.01 How can we get around this? By using the Override syntax. Override \u00b6 Overriding simple options requires no special syntax. For example, the following will change save_interval from its default value of 2 to 5: --save_interval 5 However, for complex options (i.e. nested dictionaries) the ~OVERRIDE~ flag is required to avoid merges. Let's consider the same optimizer config file from above: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 To instead use Adam with lr = 0.01 : --optimizers~OVERRIDE~ { trunk_optimizer: { Adam: { lr: 0 .01 }}} Now the config file looks like this: optimizers : trunk_optimizer : Adam : lr : 0.01 The ~OVERRIDE~ flag can be used at any level of the dictionary, which comes in handy for more complex config options. Consider this config file: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 embedder_optimizer : Adam : lr : 0.01 We can make trunk_optimizer use Adam, but leave embedder_optimizer unchanged, by applying the ~OVERRIDE~ flag to trunk_optimizer : --optimizers { trunk_optimizer~OVERRIDE~: { Adam: { lr: 0 .01 }}} Apply \u00b6 Sometimes the merging and override capabilities don't offer enough flexibility. Consider this config file: trainer : MetricLossOnly : dataloader_num_workers : 2 batch_size : 32 If we want to change the batch size using merging: --trainer: { MetricLossOnly: { batch_size: 256 }} There are two problems with this: It's verbose. We only wanted to change batch_size , but we had to write out the name of the trainer . It requires knowledge of the trainer that is being used. So instead, we can use the ~APPLY~ flag: --trainer~APPLY~2: { batch_size: 256 } This syntax means that {batch_size: 256} will be applied to (i.e. merged into) all dictionaries at a depth of 2. So the trainer config now looks like: trainer : MetricLossOnly : dataloader_num_workers : 2 batch_size : 256 Here's another example with optimizers. The starting configuration looks like: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 embedder_optimizer : Adam : lr : 0.01 We can set both learning rates to 0.005: --optimizers~APPLY~3 { lr: 0 .005 } The new config file looks like: optimizers : trunk_optimizer : RMSprop : lr : 0.005 embedder_optimizer : Adam : lr : 0.005 Swap \u00b6 Consider the trainer config file again, but with more of its parameters listed: trainer : MetricLossOnly : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : null set_min_label_to_zero : True Let's say you write your own custom trainer, and it has the same set of initialization parameters. One way to use your custom trainer is to use the ~OVERRIDE~ flag: --trainer~OVERRIDE~ { YourCustomTrainer: { iterations_per_epoch: 100 , \\ dataloader_num_workers: 32 , \\ batch_size: 32 , \\ freeze_trunk_batchnorm: True, \\ label_hierarchy_level: 0 , \\ loss_weights: null, \\ set_min_label_to_zero: True }} Again, this is very verbose, considering we only wanted to change the trainer type. So instead, we can use the ~SWAP~ flag: --trainer~SWAP~1 { YourCustomTrainer: {}} This goes to a dictionary depth of 1, and swaps the only key, MetricLossOnly , with YourCustomTrainer , while leaving everything else unchanged. Now the config file looks like: trainer : YourCustomTrainer : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : null set_min_label_to_zero : True What if there are multiple keys at the specified depth? For example, consider this configuration for data transforms: transforms : train : Resize : size : 256 RandomResizedCrop : scale : 0.16 1 ratio : 0.75 1.33 size : 227 RandomHorizontalFlip : p : 0.5 If we want to swap RandomHorizontalFlip out for RandomVerticalFlip , we need to explicitly indicate the mapping, because there are 2 other keys that could be swapped out ( Resize and RandomResizedCrop ): --trainer~SWAP~2 { RandomHorizontalFlip: RandomVerticalFlip } The new config file contains RandomVerticalFlip in place of RandomHorizontalFlip : transforms : train : Resize : size : 256 RandomResizedCrop : scale : 0.16 1 ratio : 0.75 1.33 size : 227 RandomVerticalFlip : p : 0.5 Delete \u00b6 Consider this models config file: models : trunk : bninception : pretrained : imagenet embedder : MLP : layer_sizes : - 128 Let's replace embedder with Identity() , which is a essentially an empty PyTorch module: --models { embedder~OVERRIDE~ { Identity: {}}} But because embedder has no optimizable parameters, we need to get rid of the embedder_optimizer that is specified in the default config file. We can do this easily with the ~DELETE~ flag: --optimizers { embedder_optimizer~DELETE~: {}}","title":"Command Line Syntax"},{"location":"cl_syntax/#command-line-syntax","text":"This library comes with a powerful command line syntax that makes it easy to change complex configuration options in a precise fashion.","title":"Command Line Syntax"},{"location":"cl_syntax/#lists-and-dictionaries","text":"Lists and dictionaries are written at the command line in python form: Example list: --splits_to_eval [ train, val, test ] Example nested dictionary --mining_funcs { tuple_miner: { MultiSimilarityMiner: { epsilon: 0 .1 }}}","title":"Lists and dictionaries"},{"location":"cl_syntax/#merge","text":"Consider the following optimizer configuration. optimizers : trunk_optimizer : RMSprop : lr : 0.000001 At the command line, we can change lr to 0.01, and add alpha = 0.95 to the RMSprop parameters: --optimizers { trunk_optimizer: { RMSprop: { lr: 0 .01, alpha: 0 .95 }}} So in effect, the config file now looks like this: optimizers : trunk_optimizer : RMSprop : lr : 0.01 alpha : 0.95 In other words, we specify a dictionary at the command line, using python dictionary syntax. This dictionary is then merged into the one specified in the config file. Thus, adding keys is very straightforward: --optimizers { embedder_optimizer: { Adam: { lr: 0 .01 }}} Now the config file includes a specification for embedder_optimizer : optimizers : trunk_optimizer : RMSprop : lr : 0.000001 embedder_optimizer : Adam : lr : 0.01 But what happens if we try to set the trunk_optimizer to Adam ? --optimizers { trunk_optimizer: { Adam: { lr: 0 .01 }}} Now there's a problem with the config file, because two optimizer types are specified for a single optimizer: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 Adam : lr : 0.01 How can we get around this? By using the Override syntax.","title":"Merge"},{"location":"cl_syntax/#override","text":"Overriding simple options requires no special syntax. For example, the following will change save_interval from its default value of 2 to 5: --save_interval 5 However, for complex options (i.e. nested dictionaries) the ~OVERRIDE~ flag is required to avoid merges. Let's consider the same optimizer config file from above: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 To instead use Adam with lr = 0.01 : --optimizers~OVERRIDE~ { trunk_optimizer: { Adam: { lr: 0 .01 }}} Now the config file looks like this: optimizers : trunk_optimizer : Adam : lr : 0.01 The ~OVERRIDE~ flag can be used at any level of the dictionary, which comes in handy for more complex config options. Consider this config file: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 embedder_optimizer : Adam : lr : 0.01 We can make trunk_optimizer use Adam, but leave embedder_optimizer unchanged, by applying the ~OVERRIDE~ flag to trunk_optimizer : --optimizers { trunk_optimizer~OVERRIDE~: { Adam: { lr: 0 .01 }}}","title":"Override"},{"location":"cl_syntax/#apply","text":"Sometimes the merging and override capabilities don't offer enough flexibility. Consider this config file: trainer : MetricLossOnly : dataloader_num_workers : 2 batch_size : 32 If we want to change the batch size using merging: --trainer: { MetricLossOnly: { batch_size: 256 }} There are two problems with this: It's verbose. We only wanted to change batch_size , but we had to write out the name of the trainer . It requires knowledge of the trainer that is being used. So instead, we can use the ~APPLY~ flag: --trainer~APPLY~2: { batch_size: 256 } This syntax means that {batch_size: 256} will be applied to (i.e. merged into) all dictionaries at a depth of 2. So the trainer config now looks like: trainer : MetricLossOnly : dataloader_num_workers : 2 batch_size : 256 Here's another example with optimizers. The starting configuration looks like: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 embedder_optimizer : Adam : lr : 0.01 We can set both learning rates to 0.005: --optimizers~APPLY~3 { lr: 0 .005 } The new config file looks like: optimizers : trunk_optimizer : RMSprop : lr : 0.005 embedder_optimizer : Adam : lr : 0.005","title":"Apply"},{"location":"cl_syntax/#swap","text":"Consider the trainer config file again, but with more of its parameters listed: trainer : MetricLossOnly : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : null set_min_label_to_zero : True Let's say you write your own custom trainer, and it has the same set of initialization parameters. One way to use your custom trainer is to use the ~OVERRIDE~ flag: --trainer~OVERRIDE~ { YourCustomTrainer: { iterations_per_epoch: 100 , \\ dataloader_num_workers: 32 , \\ batch_size: 32 , \\ freeze_trunk_batchnorm: True, \\ label_hierarchy_level: 0 , \\ loss_weights: null, \\ set_min_label_to_zero: True }} Again, this is very verbose, considering we only wanted to change the trainer type. So instead, we can use the ~SWAP~ flag: --trainer~SWAP~1 { YourCustomTrainer: {}} This goes to a dictionary depth of 1, and swaps the only key, MetricLossOnly , with YourCustomTrainer , while leaving everything else unchanged. Now the config file looks like: trainer : YourCustomTrainer : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : null set_min_label_to_zero : True What if there are multiple keys at the specified depth? For example, consider this configuration for data transforms: transforms : train : Resize : size : 256 RandomResizedCrop : scale : 0.16 1 ratio : 0.75 1.33 size : 227 RandomHorizontalFlip : p : 0.5 If we want to swap RandomHorizontalFlip out for RandomVerticalFlip , we need to explicitly indicate the mapping, because there are 2 other keys that could be swapped out ( Resize and RandomResizedCrop ): --trainer~SWAP~2 { RandomHorizontalFlip: RandomVerticalFlip } The new config file contains RandomVerticalFlip in place of RandomHorizontalFlip : transforms : train : Resize : size : 256 RandomResizedCrop : scale : 0.16 1 ratio : 0.75 1.33 size : 227 RandomVerticalFlip : p : 0.5","title":"Swap"},{"location":"cl_syntax/#delete","text":"Consider this models config file: models : trunk : bninception : pretrained : imagenet embedder : MLP : layer_sizes : - 128 Let's replace embedder with Identity() , which is a essentially an empty PyTorch module: --models { embedder~OVERRIDE~ { Identity: {}}} But because embedder has no optimizable parameters, we need to get rid of the embedder_optimizer that is specified in the default config file. We can do this easily with the ~DELETE~ flag: --optimizers { embedder_optimizer~DELETE~: {}}","title":"Delete"},{"location":"custom/","text":"Adding Custom Modules \u00b6 Register your own classes and modules \u00b6 By default, this library gives you access to various classes in pytorch-metric-learning, torch, torchvision, and pretrainedmodels . Let's say you want to use your own loss function as well as a custom optimizer that isn't available in torch.optim. You can accomplish this by replacing the last two lines of the example script with this: from your_own_loss import YourLossFunction from custom_optimizer import CoolOptimizer r = runner ( ** ( args . __dict__ )) # make the runner aware of them r . register ( \"loss\" , YourLossFunction ) r . register ( \"optimizer\" , CoolOptimizer ) r . run () Now you can access your custom classes just like any other class: loss_funcs : metric_loss : YourLossFunction : optimizers : trunk_optimizer : CoolOptimizer : lr : 0.01 If you have a module containing multiple classes and you want to register all those classes, you can simply register the module: import YourModuleOfLosses r . register ( \"loss\" , YourModuleOfLosses ) Registering your own trainer is a bit more involved, because you need to also create an associated API parser. The name of the api parser should be APIParser<name of your training method> . Here's an example where I make a trainer that extends trainers.MetricLossOnly , and takes in an additional argument foo . If foo is a simple parameter that can be specified directly in a config file, then APIYourTrainer doesn't need to do anything other than exist: from pytorch_metric_learning import trainers from powerful_benchmarker import api_parsers class YourTrainer ( trainers . MetricLossOnly ): def __init__ ( self , foo , ** kwargs ): super () . __init__ ( ** kwargs ) self . foo = foo print ( \"foo = \" , self . foo ) class APIYourTrainer ( api_parsers . BaseAPIParser ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) r = runner ( ** ( args . __dict__ )) r . register ( \"trainer\" , YourTrainer ) r . register ( \"api_parser\" , APIYourTrainer ) r . run () However, if foo is more complex, e.g. it is an object that requires some logic to be created, then you'll want APIYourTrainer to handle that logic, and then add foo to the default_kwargs_trainer dictionary. Check out the code documentation for details on this.","title":"Adding Custom Modules"},{"location":"custom/#adding-custom-modules","text":"","title":"Adding Custom Modules"},{"location":"custom/#register-your-own-classes-and-modules","text":"By default, this library gives you access to various classes in pytorch-metric-learning, torch, torchvision, and pretrainedmodels . Let's say you want to use your own loss function as well as a custom optimizer that isn't available in torch.optim. You can accomplish this by replacing the last two lines of the example script with this: from your_own_loss import YourLossFunction from custom_optimizer import CoolOptimizer r = runner ( ** ( args . __dict__ )) # make the runner aware of them r . register ( \"loss\" , YourLossFunction ) r . register ( \"optimizer\" , CoolOptimizer ) r . run () Now you can access your custom classes just like any other class: loss_funcs : metric_loss : YourLossFunction : optimizers : trunk_optimizer : CoolOptimizer : lr : 0.01 If you have a module containing multiple classes and you want to register all those classes, you can simply register the module: import YourModuleOfLosses r . register ( \"loss\" , YourModuleOfLosses ) Registering your own trainer is a bit more involved, because you need to also create an associated API parser. The name of the api parser should be APIParser<name of your training method> . Here's an example where I make a trainer that extends trainers.MetricLossOnly , and takes in an additional argument foo . If foo is a simple parameter that can be specified directly in a config file, then APIYourTrainer doesn't need to do anything other than exist: from pytorch_metric_learning import trainers from powerful_benchmarker import api_parsers class YourTrainer ( trainers . MetricLossOnly ): def __init__ ( self , foo , ** kwargs ): super () . __init__ ( ** kwargs ) self . foo = foo print ( \"foo = \" , self . foo ) class APIYourTrainer ( api_parsers . BaseAPIParser ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) r = runner ( ** ( args . __dict__ )) r . register ( \"trainer\" , YourTrainer ) r . register ( \"api_parser\" , APIYourTrainer ) r . run () However, if foo is more complex, e.g. it is an object that requires some logic to be created, then you'll want APIYourTrainer to handle that logic, and then add foo to the default_kwargs_trainer dictionary. Check out the code documentation for details on this.","title":"Register your own classes and modules"},{"location":"hyperparams/","text":"Hyperparameter optimization \u00b6 Bayesian optimization \u00b6 Syntax \u00b6 To tune hyperparameters using bayesian optimization: In your config files or at the command line, append ~BAYESIAN~ to any parameter that you want to tune, followed by a lower and upper bound in square brackets. Use ~LOG_BAYESIAN~ for log-scaled parameters, and ~INT_BAYESIAN~ for integer parameters. Specify the number of bayesian optimization iterations with the --bayes_opt_iters command line flag. Here is an example script which uses bayesian optimization to tune 3 hyperparameters for the multi similarity loss. python run.py --bayes_opt_iters 50 \\ --loss_funcs~OVERRIDE~ { metric_loss: { MultiSimilarityLoss: { alpha~LOG_BAYESIAN~: [ 0 .01, 100 ] , beta~LOG_BAYESIAN~: [ 0 .01, 100 ] , base~BAYESIAN~: [ 0 , 1 ]}}} \\ --experiment_name cub_bayes_opt \\ Resume optimization \u00b6 If you stop and want to resume bayesian optimization, simply run run.py with the same experiment_name you were using before. Change optimization bounds \u00b6 You can change the optimization bounds when resuming, by either changing the bounds in the config files or at the command line. The command line is preferable, because any config diffs will be recorded (just like in regular experiments ). If you're using the command line, make sure to also use the --merge_argparse_when_resuming flag. Run reproductions \u00b6 You can run a number of reproductions for the best parameters, so that you can obtain a confidence interval for your results. Use the reproductions flag, and pass in the number of reproductions you want to perform at the end of bayesian optimization. python run.py --bayes_opt_iters 50 --reproductions 10 \\ --experiment_name cub_bayes_opt \\","title":"Hyperparameter Optimization"},{"location":"hyperparams/#hyperparameter-optimization","text":"","title":"Hyperparameter optimization"},{"location":"hyperparams/#bayesian-optimization","text":"","title":"Bayesian optimization"},{"location":"hyperparams/#syntax","text":"To tune hyperparameters using bayesian optimization: In your config files or at the command line, append ~BAYESIAN~ to any parameter that you want to tune, followed by a lower and upper bound in square brackets. Use ~LOG_BAYESIAN~ for log-scaled parameters, and ~INT_BAYESIAN~ for integer parameters. Specify the number of bayesian optimization iterations with the --bayes_opt_iters command line flag. Here is an example script which uses bayesian optimization to tune 3 hyperparameters for the multi similarity loss. python run.py --bayes_opt_iters 50 \\ --loss_funcs~OVERRIDE~ { metric_loss: { MultiSimilarityLoss: { alpha~LOG_BAYESIAN~: [ 0 .01, 100 ] , beta~LOG_BAYESIAN~: [ 0 .01, 100 ] , base~BAYESIAN~: [ 0 , 1 ]}}} \\ --experiment_name cub_bayes_opt \\","title":"Syntax"},{"location":"hyperparams/#resume-optimization","text":"If you stop and want to resume bayesian optimization, simply run run.py with the same experiment_name you were using before.","title":"Resume optimization"},{"location":"hyperparams/#change-optimization-bounds","text":"You can change the optimization bounds when resuming, by either changing the bounds in the config files or at the command line. The command line is preferable, because any config diffs will be recorded (just like in regular experiments ). If you're using the command line, make sure to also use the --merge_argparse_when_resuming flag.","title":"Change optimization bounds"},{"location":"hyperparams/#run-reproductions","text":"You can run a number of reproductions for the best parameters, so that you can obtain a confidence interval for your results. Use the reproductions flag, and pass in the number of reproductions you want to perform at the end of bayesian optimization. python run.py --bayes_opt_iters 50 --reproductions 10 \\ --experiment_name cub_bayes_opt \\","title":"Run reproductions"},{"location":"yaml_syntax/","text":"Yaml Syntax \u00b6 Config files in this library are yaml files, so they follow standard yaml syntax. But you can also use all of the command line syntax within your config files. For example, here are two config files in the config_general category: default.yaml trainer : MetricLossOnly : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : null set_min_label_to_zero : True num_epochs_train : 1000 save_interval : 2 patience : 9 check_untrained_accuracy : True skip_eval_if_already_done : True skip_ensemble_eval_if_already_done : True save_figures_on_tensorboard : False save_lists_in_db : False override_required_compatible_factories : False with_daml.yaml trainer~SWAP~1 : DeepAdversarialMetricLearning : trainer~APPLY~2 : g_alone_epochs : 0 metric_alone_epochs : 0 g_triplets_per_anchor : 100 loss_weights : metric_loss : 1 synth_loss : 0.1 g_adv_loss : 0.1 g_hard_loss : 0.1 g_reg_loss : 0.1 The with_daml config file contains the special flags ~SWAP~ and ~APPLY~ . This particular config file won't work by itself. However, it can be loaded in conjunction with default at the command line: --config_general [ default, with_daml ] This loads default.yaml , followed by with_daml.yaml . Now the special ~SWAP~ and ~APPLY~ flags will have an effect. Specifically, MetricLossOnly will get swapped out for DeepAdversarialMetricLearning , and then the parameters for DeepAdversarialMetricLearning will be applied to the trainer dictionary. The final config file ends up looking like this: trainer : DeepAdversarialMetricLearning : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : metric_loss : 1 synth_loss : 0.1 g_adv_loss : 0.1 g_hard_loss : 0.1 g_reg_loss : 0.1 set_min_label_to_zero : True g_alone_epochs : 0 metric_alone_epochs : 0 g_triplets_per_anchor : 100 num_epochs_train : 1000 save_interval : 2 patience : 9 check_untrained_accuracy : True skip_eval_if_already_done : True skip_ensemble_eval_if_already_done : True save_figures_on_tensorboard : False save_lists_in_db : False override_required_compatible_factories : False","title":"Yaml Syntax"},{"location":"yaml_syntax/#yaml-syntax","text":"Config files in this library are yaml files, so they follow standard yaml syntax. But you can also use all of the command line syntax within your config files. For example, here are two config files in the config_general category: default.yaml trainer : MetricLossOnly : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : null set_min_label_to_zero : True num_epochs_train : 1000 save_interval : 2 patience : 9 check_untrained_accuracy : True skip_eval_if_already_done : True skip_ensemble_eval_if_already_done : True save_figures_on_tensorboard : False save_lists_in_db : False override_required_compatible_factories : False with_daml.yaml trainer~SWAP~1 : DeepAdversarialMetricLearning : trainer~APPLY~2 : g_alone_epochs : 0 metric_alone_epochs : 0 g_triplets_per_anchor : 100 loss_weights : metric_loss : 1 synth_loss : 0.1 g_adv_loss : 0.1 g_hard_loss : 0.1 g_reg_loss : 0.1 The with_daml config file contains the special flags ~SWAP~ and ~APPLY~ . This particular config file won't work by itself. However, it can be loaded in conjunction with default at the command line: --config_general [ default, with_daml ] This loads default.yaml , followed by with_daml.yaml . Now the special ~SWAP~ and ~APPLY~ flags will have an effect. Specifically, MetricLossOnly will get swapped out for DeepAdversarialMetricLearning , and then the parameters for DeepAdversarialMetricLearning will be applied to the trainer dictionary. The final config file ends up looking like this: trainer : DeepAdversarialMetricLearning : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : metric_loss : 1 synth_loss : 0.1 g_adv_loss : 0.1 g_hard_loss : 0.1 g_reg_loss : 0.1 set_min_label_to_zero : True g_alone_epochs : 0 metric_alone_epochs : 0 g_triplets_per_anchor : 100 num_epochs_train : 1000 save_interval : 2 patience : 9 check_untrained_accuracy : True skip_eval_if_already_done : True skip_ensemble_eval_if_already_done : True save_figures_on_tensorboard : False save_lists_in_db : False override_required_compatible_factories : False","title":"Yaml Syntax"},{"location":"code/aggregators/","text":"Aggregators \u00b6 MeanAggregator \u00b6","title":"Aggregators"},{"location":"code/aggregators/#aggregators","text":"","title":"Aggregators"},{"location":"code/aggregators/#meanaggregator","text":"","title":"MeanAggregator"},{"location":"code/api_parsers/","text":"API Parsers \u00b6 BaseAPIParser \u00b6","title":"API Parsers"},{"location":"code/api_parsers/#api-parsers","text":"","title":"API Parsers"},{"location":"code/api_parsers/#baseapiparser","text":"","title":"BaseAPIParser"},{"location":"code/architectures/","text":"Architectures \u00b6 ListOfModels \u00b6 MLP \u00b6","title":"Architectures"},{"location":"code/architectures/#architectures","text":"","title":"Architectures"},{"location":"code/architectures/#listofmodels","text":"","title":"ListOfModels"},{"location":"code/architectures/#mlp","text":"","title":"MLP"},{"location":"code/datasets/","text":"Datasets \u00b6 Cars196 \u00b6 CUB200 \u00b6 StanfordOnlineProducts \u00b6","title":"Datasets"},{"location":"code/datasets/#datasets","text":"","title":"Datasets"},{"location":"code/datasets/#cars196","text":"","title":"Cars196"},{"location":"code/datasets/#cub200","text":"","title":"CUB200"},{"location":"code/datasets/#stanfordonlineproducts","text":"","title":"StanfordOnlineProducts"},{"location":"code/ensembles/","text":"Ensembles \u00b6 ConcatenateEmbeddings \u00b6","title":"Ensembles"},{"location":"code/ensembles/#ensembles","text":"","title":"Ensembles"},{"location":"code/ensembles/#concatenateembeddings","text":"","title":"ConcatenateEmbeddings"},{"location":"code/factories/","text":"Factories \u00b6","title":"Factories"},{"location":"code/factories/#factories","text":"","title":"Factories"},{"location":"code/runners/","text":"Runners \u00b6 BaseRunner \u00b6 BayesOptRunner \u00b6 SingleExperimentRunner \u00b6","title":"Runners"},{"location":"code/runners/#runners","text":"","title":"Runners"},{"location":"code/runners/#baserunner","text":"","title":"BaseRunner"},{"location":"code/runners/#bayesoptrunner","text":"","title":"BayesOptRunner"},{"location":"code/runners/#singleexperimentrunner","text":"","title":"SingleExperimentRunner"},{"location":"code/split_managers/","text":"Split Managers \u00b6 BaseSplitManager \u00b6 ClassDisjointSplitManager \u00b6 ClosedSetSplitManager \u00b6 IndexSplitManager \u00b6 PredefinedSplitManager \u00b6 SplitSchemeHolder \u00b6","title":"Split Managers"},{"location":"code/split_managers/#split-managers","text":"","title":"Split Managers"},{"location":"code/split_managers/#basesplitmanager","text":"","title":"BaseSplitManager"},{"location":"code/split_managers/#classdisjointsplitmanager","text":"","title":"ClassDisjointSplitManager"},{"location":"code/split_managers/#closedsetsplitmanager","text":"","title":"ClosedSetSplitManager"},{"location":"code/split_managers/#indexsplitmanager","text":"","title":"IndexSplitManager"},{"location":"code/split_managers/#predefinedsplitmanager","text":"","title":"PredefinedSplitManager"},{"location":"code/split_managers/#splitschemeholder","text":"","title":"SplitSchemeHolder"},{"location":"code/utils/","text":"Utils \u00b6 constants \u00b6 dataset_utils \u00b6","title":"Utils"},{"location":"code/utils/#utils","text":"","title":"Utils"},{"location":"code/utils/#constants","text":"","title":"constants"},{"location":"code/utils/#dataset_utils","text":"","title":"dataset_utils"},{"location":"configs/config_dataset/","text":"config_dataset \u00b6 dataset \u00b6 This is the dataset that will be used for training, validation, and testing. Default yaml: dataset : CUB200 : Example command line modification: # Change dataset to Cars196 --dataset~OVERRIDE~ { Cars196: {}} splits_to_eval \u00b6 The names of splits for which accuracy should be computed. Default yaml: splits_to_eval : - val Example command line modification: # Eval on train, val, and test. --splits_to_eval [ train, val, test ] split_manager \u00b6 The split manager determines how the train/val/test splits are formed. Default yaml: split_manager : ClassDisjointSplitManager : test_size : 0.5 test_start_idx : 0.5 num_training_partitions : 4 num_training_sets : 4 hierarchy_level : 0 data_and_label_getter_keys : [ data , label ] Example command line modification: # Change number of training sets to 2, and the test size to 0.3 --split_manager~APPLY~2 { test_size: 0 .3, num_training_sets: 2 }","title":"config_dataset"},{"location":"configs/config_dataset/#config_dataset","text":"","title":"config_dataset"},{"location":"configs/config_dataset/#dataset","text":"This is the dataset that will be used for training, validation, and testing. Default yaml: dataset : CUB200 : Example command line modification: # Change dataset to Cars196 --dataset~OVERRIDE~ { Cars196: {}}","title":"dataset"},{"location":"configs/config_dataset/#splits_to_eval","text":"The names of splits for which accuracy should be computed. Default yaml: splits_to_eval : - val Example command line modification: # Eval on train, val, and test. --splits_to_eval [ train, val, test ]","title":"splits_to_eval"},{"location":"configs/config_dataset/#split_manager","text":"The split manager determines how the train/val/test splits are formed. Default yaml: split_manager : ClassDisjointSplitManager : test_size : 0.5 test_start_idx : 0.5 num_training_partitions : 4 num_training_sets : 4 hierarchy_level : 0 data_and_label_getter_keys : [ data , label ] Example command line modification: # Change number of training sets to 2, and the test size to 0.3 --split_manager~APPLY~2 { test_size: 0 .3, num_training_sets: 2 }","title":"split_manager"},{"location":"configs/config_eval/","text":"config_eval \u00b6 tester \u00b6 The tester computes the accuracy of your model. Default yaml: tester : GlobalEmbeddingSpaceTester : reference_set : compared_to_self normalize_embeddings : True use_trunk_output : False batch_size : 32 dataloader_num_workers : 2 pca : null accuracy_calculator : AccuracyCalculator : label_hierarchy_level : 0 Example command line modification: # Change batch size to 256 and don't normalize embeddings --tester~APPLY~2 { batch_size: 256 , normalize_embeddings: False } aggregator \u00b6 The aggregator takes the accuracies from all the cross-validation models, and returns a single number to represent the overall performance. Default yaml: aggregator : MeanAggregator : Example command line modification: # Use your own custom aggregator --aggregator~OVERRIDE~ { YourCustomAggregator: {}} ensemble \u00b6 The ensemble combines the cross-validation models into a single model. Default yaml: ensemble : ConcatenateEmbeddings : normalize_embeddings : True use_trunk_output : False hook_container \u00b6 The hook container contains end-of-testing, end-of-epoch, and end-of-iteration hooks. It also contains a record keeper, for writing and reading to database files. Default yaml: hook_container : HookContainer : primary_metric : mean_average_precision_at_r validation_split_name : val Example command line modification: # Change the primary metric to precision_at_1 --hook_container~APPLY~2 { primary_metric: precision_at_1 }","title":"config_eval"},{"location":"configs/config_eval/#config_eval","text":"","title":"config_eval"},{"location":"configs/config_eval/#tester","text":"The tester computes the accuracy of your model. Default yaml: tester : GlobalEmbeddingSpaceTester : reference_set : compared_to_self normalize_embeddings : True use_trunk_output : False batch_size : 32 dataloader_num_workers : 2 pca : null accuracy_calculator : AccuracyCalculator : label_hierarchy_level : 0 Example command line modification: # Change batch size to 256 and don't normalize embeddings --tester~APPLY~2 { batch_size: 256 , normalize_embeddings: False }","title":"tester"},{"location":"configs/config_eval/#aggregator","text":"The aggregator takes the accuracies from all the cross-validation models, and returns a single number to represent the overall performance. Default yaml: aggregator : MeanAggregator : Example command line modification: # Use your own custom aggregator --aggregator~OVERRIDE~ { YourCustomAggregator: {}}","title":"aggregator"},{"location":"configs/config_eval/#ensemble","text":"The ensemble combines the cross-validation models into a single model. Default yaml: ensemble : ConcatenateEmbeddings : normalize_embeddings : True use_trunk_output : False","title":"ensemble"},{"location":"configs/config_eval/#hook_container","text":"The hook container contains end-of-testing, end-of-epoch, and end-of-iteration hooks. It also contains a record keeper, for writing and reading to database files. Default yaml: hook_container : HookContainer : primary_metric : mean_average_precision_at_r validation_split_name : val Example command line modification: # Change the primary metric to precision_at_1 --hook_container~APPLY~2 { primary_metric: precision_at_1 }","title":"hook_container"},{"location":"configs/config_factories/","text":"config_factories \u00b6 factories \u00b6 Factories determine how objects are constructed, based on parameters in the config files, and parameters generated within the code. Default yaml: factories : model : ModelFactory : {} loss : LossFactory : {} miner : MinerFactory : {} sampler : SamplerFactory : {} optimizer : OptimizerFactory : {} tester : TesterFactory : {} trainer : TrainerFactory : {} transform : TransformFactory : {} split_manager : SplitManagerFactory : {} record_keeper : RecordKeeperFactory : {} hook : HookFactory : {} aggregator : AggregatorFactory : {} ensemble : EnsembleFactory : {} Example command line modification: # Set the base_output_model_size manually --factories { model~APPLY~2: { base_output_model_size: 1024 }}","title":"config_factories"},{"location":"configs/config_factories/#config_factories","text":"","title":"config_factories"},{"location":"configs/config_factories/#factories","text":"Factories determine how objects are constructed, based on parameters in the config files, and parameters generated within the code. Default yaml: factories : model : ModelFactory : {} loss : LossFactory : {} miner : MinerFactory : {} sampler : SamplerFactory : {} optimizer : OptimizerFactory : {} tester : TesterFactory : {} trainer : TrainerFactory : {} transform : TransformFactory : {} split_manager : SplitManagerFactory : {} record_keeper : RecordKeeperFactory : {} hook : HookFactory : {} aggregator : AggregatorFactory : {} ensemble : EnsembleFactory : {} Example command line modification: # Set the base_output_model_size manually --factories { model~APPLY~2: { base_output_model_size: 1024 }}","title":"factories"},{"location":"configs/config_general/","text":"config_general \u00b6 trainer \u00b6 The trainer trains your model. Default yaml: trainer : MetricLossOnly : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : null set_min_label_to_zero : True Example command line modification: # Swap in a different trainer, but keep the input parameters the same --trainer~SWAP~1 { CascadedEmbeddings: null } num_epochs_train \u00b6 The maximum number of epochs to train for. Default yaml: num_epochs_train : 1000 Example command line modification: --num_epochs_train 100 save_interval \u00b6 Models will be evaluated and saved every save_interval epochs. Default yaml: save_interval : 2 Example command line modification: --save_interval 10 patience \u00b6 Training will end if the validation accuracy stops improving after patience+1 epochs. Default yaml: save_interval : 2 Example command line modification: # Don't use patience at all --patience null check_untrained_accuracy \u00b6 If True , then the tester will compute accuracy for the initial trunk (epoch -1) and initial trunk + embedder (epoch 0). Otherwise, these will be skipped. Default yaml: check_untrained_accuracy : True Example command line modification: --check_untrained_accuracy False skip_eval_if_already_done \u00b6 If True , then the tester will skip evaluation if a split/epoch has already been logged in the log files. If False , then the tester will evaluate a split/epoch regardless of whether it has already been done in the past. Previous logs will be preserved, hence the logs will contain duplicate results, and the most recent version for any split/epoch will be considered the \"official\" value for that split/epoch. Default yaml: skip_eval_if_already_done : True Example command line modification: --skip_eval_if_already_done False skip_ensemble_eval_if_already_done \u00b6 The same as skip_eval_if_already_done , but for ensembles. Default yaml: skip_ensemble_eval_if_already_done : True Example command line modification: --skip_ensemble_eval_if_already_done False save_figures_on_tensorboard \u00b6 Use matplotlib to plot things on tensorboard. (Most data doesn't require matplotlib.) Default yaml: save_figures_on_tensorboard : False Example command line modification: --save_figures_on_tensorboard True save_lists_in_db \u00b6 In record-keeper, non-scalar values are saved in the database as json-lists. This setting is False by default, because these lists can sometimes be quite large, causing the database file size to grow quickly. Default yaml: save_lists_in_db : False Example command line modification: --save_lists_in_db True override_required_compatible_factories \u00b6 Each APIParser comes with predefined compatible factories, which are used by default, regardless of what is specified in the factories config option. This allows you to specify a trainer without having to specify all the required factories. However, if you have your own custom factory that you know is compatible, and want to use that instead, you should set this flag to True. Default yaml: override_required_compatible_factories : False Example command line modification: --override_required_compatible_factories True","title":"config_general"},{"location":"configs/config_general/#config_general","text":"","title":"config_general"},{"location":"configs/config_general/#trainer","text":"The trainer trains your model. Default yaml: trainer : MetricLossOnly : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : null set_min_label_to_zero : True Example command line modification: # Swap in a different trainer, but keep the input parameters the same --trainer~SWAP~1 { CascadedEmbeddings: null }","title":"trainer"},{"location":"configs/config_general/#num_epochs_train","text":"The maximum number of epochs to train for. Default yaml: num_epochs_train : 1000 Example command line modification: --num_epochs_train 100","title":"num_epochs_train"},{"location":"configs/config_general/#save_interval","text":"Models will be evaluated and saved every save_interval epochs. Default yaml: save_interval : 2 Example command line modification: --save_interval 10","title":"save_interval"},{"location":"configs/config_general/#patience","text":"Training will end if the validation accuracy stops improving after patience+1 epochs. Default yaml: save_interval : 2 Example command line modification: # Don't use patience at all --patience null","title":"patience"},{"location":"configs/config_general/#check_untrained_accuracy","text":"If True , then the tester will compute accuracy for the initial trunk (epoch -1) and initial trunk + embedder (epoch 0). Otherwise, these will be skipped. Default yaml: check_untrained_accuracy : True Example command line modification: --check_untrained_accuracy False","title":"check_untrained_accuracy"},{"location":"configs/config_general/#skip_eval_if_already_done","text":"If True , then the tester will skip evaluation if a split/epoch has already been logged in the log files. If False , then the tester will evaluate a split/epoch regardless of whether it has already been done in the past. Previous logs will be preserved, hence the logs will contain duplicate results, and the most recent version for any split/epoch will be considered the \"official\" value for that split/epoch. Default yaml: skip_eval_if_already_done : True Example command line modification: --skip_eval_if_already_done False","title":"skip_eval_if_already_done"},{"location":"configs/config_general/#skip_ensemble_eval_if_already_done","text":"The same as skip_eval_if_already_done , but for ensembles. Default yaml: skip_ensemble_eval_if_already_done : True Example command line modification: --skip_ensemble_eval_if_already_done False","title":"skip_ensemble_eval_if_already_done"},{"location":"configs/config_general/#save_figures_on_tensorboard","text":"Use matplotlib to plot things on tensorboard. (Most data doesn't require matplotlib.) Default yaml: save_figures_on_tensorboard : False Example command line modification: --save_figures_on_tensorboard True","title":"save_figures_on_tensorboard"},{"location":"configs/config_general/#save_lists_in_db","text":"In record-keeper, non-scalar values are saved in the database as json-lists. This setting is False by default, because these lists can sometimes be quite large, causing the database file size to grow quickly. Default yaml: save_lists_in_db : False Example command line modification: --save_lists_in_db True","title":"save_lists_in_db"},{"location":"configs/config_general/#override_required_compatible_factories","text":"Each APIParser comes with predefined compatible factories, which are used by default, regardless of what is specified in the factories config option. This allows you to specify a trainer without having to specify all the required factories. However, if you have your own custom factory that you know is compatible, and want to use that instead, you should set this flag to True. Default yaml: override_required_compatible_factories : False Example command line modification: --override_required_compatible_factories True","title":"override_required_compatible_factories"},{"location":"configs/config_loss_and_miners/","text":"config_loss_and_miners \u00b6 loss_funcs \u00b6 The loss functions are given embeddings and labels, and output a value on which back propagation can be performed. This config option is a mapping from strings to loss classes. The strings should match the loss names used by your trainer. Default yaml: loss_funcs : metric_loss : ContrastiveLoss : Example command line modification: # Use a different loss function --loss_funcs { metric_loss~OVERRIDE~: { MultiSimilarityLoss: { alpha: 0 .1, beta: 40 , base: 0 .5 }}} sampler \u00b6 The sampler is passed to the PyTorch dataloader, and determines how batches are formed. Use {} if you want random sampling. Default yaml: sampler : MPerClassSampler : m : 4 Example command line modification: # Use random sampling --sampler~OVERRIDE~ {} mining_funcs \u00b6 Mining functions determine the best tuples to train on, within an arbitrarily formed batch. This config option is a mapping from strings to miner classes. The strings should match the miner names used by your trainer. Default yaml: mining_funcs : {} Example command line modification: # Use a miner --mining_funcs { tuple_miner: { MultiSimilarityMiner: { epsilon: 0 .1 }}}","title":"config_loss_and_miners"},{"location":"configs/config_loss_and_miners/#config_loss_and_miners","text":"","title":"config_loss_and_miners"},{"location":"configs/config_loss_and_miners/#loss_funcs","text":"The loss functions are given embeddings and labels, and output a value on which back propagation can be performed. This config option is a mapping from strings to loss classes. The strings should match the loss names used by your trainer. Default yaml: loss_funcs : metric_loss : ContrastiveLoss : Example command line modification: # Use a different loss function --loss_funcs { metric_loss~OVERRIDE~: { MultiSimilarityLoss: { alpha: 0 .1, beta: 40 , base: 0 .5 }}}","title":"loss_funcs"},{"location":"configs/config_loss_and_miners/#sampler","text":"The sampler is passed to the PyTorch dataloader, and determines how batches are formed. Use {} if you want random sampling. Default yaml: sampler : MPerClassSampler : m : 4 Example command line modification: # Use random sampling --sampler~OVERRIDE~ {}","title":"sampler"},{"location":"configs/config_loss_and_miners/#mining_funcs","text":"Mining functions determine the best tuples to train on, within an arbitrarily formed batch. This config option is a mapping from strings to miner classes. The strings should match the miner names used by your trainer. Default yaml: mining_funcs : {} Example command line modification: # Use a miner --mining_funcs { tuple_miner: { MultiSimilarityMiner: { epsilon: 0 .1 }}}","title":"mining_funcs"},{"location":"configs/config_models/","text":"config_models \u00b6 models \u00b6 The models take in input (like images, text etc.) and output embeddings. There is no specific requires about what the structure of the trunk and embedder. The only requirement is that the trunk's output can be fed into the embedder. For example, if you want to use the bninception model, but don't want to append any layers after it, you can set embedder to Identity . This will make the embedder's output equal to its input. Default yaml: models : trunk : bninception : pretrained : imagenet embedder : MLP : layer_sizes : - 128 Example command line modification: # Set embedder to Identity. --models { embedder~OVERRIDE~: { Identity: {}}} \\ # You'll need to delete the embedder_optimizer, because Identity() has no parameters --optimizers { embedder_optimizer~DELETE~: null }","title":"config_models"},{"location":"configs/config_models/#config_models","text":"","title":"config_models"},{"location":"configs/config_models/#models","text":"The models take in input (like images, text etc.) and output embeddings. There is no specific requires about what the structure of the trunk and embedder. The only requirement is that the trunk's output can be fed into the embedder. For example, if you want to use the bninception model, but don't want to append any layers after it, you can set embedder to Identity . This will make the embedder's output equal to its input. Default yaml: models : trunk : bninception : pretrained : imagenet embedder : MLP : layer_sizes : - 128 Example command line modification: # Set embedder to Identity. --models { embedder~OVERRIDE~: { Identity: {}}} \\ # You'll need to delete the embedder_optimizer, because Identity() has no parameters --optimizers { embedder_optimizer~DELETE~: null }","title":"models"},{"location":"configs/config_optimizers/","text":"config_optimizers \u00b6 optimizers \u00b6 Optimizers determine how your model weights are updated. This config option maps from strings to optimizer classes. Each string should have the form <model_name>_optimizer . Default yaml: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 weight_decay : 0.0001 momentum : 0.9 embedder_optimizer : RMSprop : lr : 0.000001 weight_decay : 0.0001 momentum : 0.9 Example command line modification: # Change the learning rate to 0.01, for both the trunk and embedder --optimizers~APPLY~3 { lr: 0 .01 }","title":"config_optimizers"},{"location":"configs/config_optimizers/#config_optimizers","text":"","title":"config_optimizers"},{"location":"configs/config_optimizers/#optimizers","text":"Optimizers determine how your model weights are updated. This config option maps from strings to optimizer classes. Each string should have the form <model_name>_optimizer . Default yaml: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 weight_decay : 0.0001 momentum : 0.9 embedder_optimizer : RMSprop : lr : 0.000001 weight_decay : 0.0001 momentum : 0.9 Example command line modification: # Change the learning rate to 0.01, for both the trunk and embedder --optimizers~APPLY~3 { lr: 0 .01 }","title":"optimizers"},{"location":"configs/config_transforms/","text":"config_transforms \u00b6 transforms \u00b6 Specifies the transforms to be used during training and during evaluation. ToTensor() and Normalize do not need to be specified, as they are added by default. Default yaml: transforms : train : Resize : size : 256 RandomResizedCrop : scale : 0.16 1 ratio : 0.75 1.33 size : 227 RandomHorizontalFlip : p : 0.5 eval : Resize : size : 256 CenterCrop : size : 227 Example command line modification: # Use RandomVerticalFlip instead of RandomHorizontalFlip --transforms { train~SWAP~1: { RandomHorizontalFlip: RandomVerticalFlip }}","title":"config_transforms"},{"location":"configs/config_transforms/#config_transforms","text":"","title":"config_transforms"},{"location":"configs/config_transforms/#transforms","text":"Specifies the transforms to be used during training and during evaluation. ToTensor() and Normalize do not need to be specified, as they are added by default. Default yaml: transforms : train : Resize : size : 256 RandomResizedCrop : scale : 0.16 1 ratio : 0.75 1.33 size : 227 RandomHorizontalFlip : p : 0.5 eval : Resize : size : 256 CenterCrop : size : 227 Example command line modification: # Use RandomVerticalFlip instead of RandomHorizontalFlip --transforms { train~SWAP~1: { RandomHorizontalFlip: RandomVerticalFlip }}","title":"transforms"},{"location":"papers/mlrc/","text":"A Metric Learning Reality Check \u00b6 This page contains additional information for the ECCV 2020 paper by Musgrave et al. Optimization plots \u00b6 Click on the links below to view the bayesian optimization plots CUB200 Cars196 SOP CUB200 with Batch 256 Contrastive Contrastive Contrastive Contrastive Triplet Triplet Triplet Triplet NTXent NTXent NTXent NTXent ProxyNCA ProxyNCA ProxyNCA ProxyNCA Margin Margin Margin Margin Margin / class Margin / class Margin / class Margin / class Normalized Softmax Normalized Softmax Normalized Softmax Normalized Softmax CosFace CosFace CosFace CosFace ArcFace ArcFace ArcFace ArcFace FastAP FastAP FastAP FastAP SNR Contrastive SNR Contrastive SNR Contrastive SNR Contrastive Multi Similarity Multi Similarity Multi Similarity Multi Similarity Multi Similarity + Miner Multi Similarity + Miner Multi Similarity + Miner Multi Similarity + Miner SoftTriple SoftTriple SoftTriple SoftTriple Reproducing results \u00b6 Download the experiment folder \u00b6 Go to the benchmark spreadsheet Find the experiment you want to reproduce, and click on the link in the \"Config files\" column. You'll see 3 folders: one for CUB, one for Cars, and one for SOP. Open the folder for the dataset you want to train on. Now you'll see several files and folders, one of which ends in \"reproduction0\". Download this folder. (It will include saved models. If you don't want to download the saved models, go into the folder and download just the \"configs\" folder.) Command line scripts \u00b6 Normally reproducing results is as easy as downloading an experiment folder, and using the reproduce_results flag . However, there have been significant changes to the API since these experiments were run, so there are a couple of extra steps required, and they depend on the dataset. In the following code, <experiment_to_reproduce> refers to the folder that contains the configs folder. CUB200: python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> Cars196: python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> \\ --config_dataset [ default, with_cars196 ] \\ --config_general [ default, with_cars196 ] \\ --merge_argparse_when_resuming Stanford Online Products python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> \\ --config_dataset [ default, with_sop ] \\ --config_general [ default, with_sop ] \\ --merge_argparse_when_resuming CUB200 with batch size 256: python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> \\ --trainer~APPLY~2: { batch_size: 256 } If you don't have the datasets and would like to download them into your dataset_root folder, you can add this flag to the CUB commands: --dataset~OVERRIDE~ { CUB200: { download: True }} Likewise, for the Cars196 and Stanford Online Products commands, replace the --config_dataset flag with: --dataset~OVERRIDE~ { Cars196: { download: True }} or --dataset~OVERRIDE~ { StanfordOnlineProducts: { download: True }} Frequently Asked Questions \u00b6 Isn't it unfair to fix the model, optimizer, learning rate, and embedding size? \u00b6 Our goal was to compare algorithms fairly. To accomplish this, we used the same network, optimizer, learning rate, image transforms, and embedding dimensionality for each algorithm. There is no theoretical reason why changing any of these parameters would benefit one particular algorithm over the rest. If there is no theoretical reason, then we can only speculate, and if we add hyperparameters based on speculation, then the search space becomes too large to explore. Why was the batch size set to 32 for most of the results? \u00b6 This was done for the sake of computational efficiency. Note that there are: 3 datasets 14 algorithms 50 steps of bayesian optmization 4 fold cross validation This comes to 8400 models to train, which can take a considerable amount of time. Thus, a batch size of 32 made sense. It's also important to remember that there are real-world cases where a large batch size cannot be used. For example, if you want to train on large images, rather than the contrived case of 227x227, then training with a batch size of 32 suddenly makes a lot more sense because you are constrained by GPU memory. So it's reasonable to check the performance of these losses on a batch size of 32. That said, there is a good theoretical reason for a larger batch size benefiting embedding losses more than classification losses. Specifically, embedding losses can benefit from the increased number of pairs/triplets in larger batches. To address this, we benchmarked the 14 methods on CUB200, using a batch size of 256. The results can be found in the supplementary section (the final page) of the paper. Why weren't more hard-mining methods evaluated? \u00b6 We did test one loss+miner combination (Multi-similarity loss + their mining method). But we mainly wanted to do a thorough evaluation of loss functions, because that is the subject of most recent metric learning papers. For the contrastive loss, why is the optimal positive margin a negative value? \u00b6 A negative value should be equivalent to a margin of 0, because the distance between positive pairs cannot be negative, and the margin does not contribute to the gradient. So allowing the hyperparameter optimization to explore negative margins was unnecesary, but by the time I realized this, it wasn't worth changing the optimization bounds.","title":"A Metric Learning Reality Check"},{"location":"papers/mlrc/#a-metric-learning-reality-check","text":"This page contains additional information for the ECCV 2020 paper by Musgrave et al.","title":"A Metric Learning Reality Check"},{"location":"papers/mlrc/#optimization-plots","text":"Click on the links below to view the bayesian optimization plots CUB200 Cars196 SOP CUB200 with Batch 256 Contrastive Contrastive Contrastive Contrastive Triplet Triplet Triplet Triplet NTXent NTXent NTXent NTXent ProxyNCA ProxyNCA ProxyNCA ProxyNCA Margin Margin Margin Margin Margin / class Margin / class Margin / class Margin / class Normalized Softmax Normalized Softmax Normalized Softmax Normalized Softmax CosFace CosFace CosFace CosFace ArcFace ArcFace ArcFace ArcFace FastAP FastAP FastAP FastAP SNR Contrastive SNR Contrastive SNR Contrastive SNR Contrastive Multi Similarity Multi Similarity Multi Similarity Multi Similarity Multi Similarity + Miner Multi Similarity + Miner Multi Similarity + Miner Multi Similarity + Miner SoftTriple SoftTriple SoftTriple SoftTriple","title":"Optimization plots"},{"location":"papers/mlrc/#reproducing-results","text":"","title":"Reproducing results"},{"location":"papers/mlrc/#download-the-experiment-folder","text":"Go to the benchmark spreadsheet Find the experiment you want to reproduce, and click on the link in the \"Config files\" column. You'll see 3 folders: one for CUB, one for Cars, and one for SOP. Open the folder for the dataset you want to train on. Now you'll see several files and folders, one of which ends in \"reproduction0\". Download this folder. (It will include saved models. If you don't want to download the saved models, go into the folder and download just the \"configs\" folder.)","title":"Download the experiment folder"},{"location":"papers/mlrc/#command-line-scripts","text":"Normally reproducing results is as easy as downloading an experiment folder, and using the reproduce_results flag . However, there have been significant changes to the API since these experiments were run, so there are a couple of extra steps required, and they depend on the dataset. In the following code, <experiment_to_reproduce> refers to the folder that contains the configs folder. CUB200: python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> Cars196: python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> \\ --config_dataset [ default, with_cars196 ] \\ --config_general [ default, with_cars196 ] \\ --merge_argparse_when_resuming Stanford Online Products python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> \\ --config_dataset [ default, with_sop ] \\ --config_general [ default, with_sop ] \\ --merge_argparse_when_resuming CUB200 with batch size 256: python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> \\ --trainer~APPLY~2: { batch_size: 256 } If you don't have the datasets and would like to download them into your dataset_root folder, you can add this flag to the CUB commands: --dataset~OVERRIDE~ { CUB200: { download: True }} Likewise, for the Cars196 and Stanford Online Products commands, replace the --config_dataset flag with: --dataset~OVERRIDE~ { Cars196: { download: True }} or --dataset~OVERRIDE~ { StanfordOnlineProducts: { download: True }}","title":"Command line scripts"},{"location":"papers/mlrc/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"papers/mlrc/#isnt-it-unfair-to-fix-the-model-optimizer-learning-rate-and-embedding-size","text":"Our goal was to compare algorithms fairly. To accomplish this, we used the same network, optimizer, learning rate, image transforms, and embedding dimensionality for each algorithm. There is no theoretical reason why changing any of these parameters would benefit one particular algorithm over the rest. If there is no theoretical reason, then we can only speculate, and if we add hyperparameters based on speculation, then the search space becomes too large to explore.","title":"Isn't it unfair to fix the model, optimizer, learning rate, and embedding size?"},{"location":"papers/mlrc/#why-was-the-batch-size-set-to-32-for-most-of-the-results","text":"This was done for the sake of computational efficiency. Note that there are: 3 datasets 14 algorithms 50 steps of bayesian optmization 4 fold cross validation This comes to 8400 models to train, which can take a considerable amount of time. Thus, a batch size of 32 made sense. It's also important to remember that there are real-world cases where a large batch size cannot be used. For example, if you want to train on large images, rather than the contrived case of 227x227, then training with a batch size of 32 suddenly makes a lot more sense because you are constrained by GPU memory. So it's reasonable to check the performance of these losses on a batch size of 32. That said, there is a good theoretical reason for a larger batch size benefiting embedding losses more than classification losses. Specifically, embedding losses can benefit from the increased number of pairs/triplets in larger batches. To address this, we benchmarked the 14 methods on CUB200, using a batch size of 256. The results can be found in the supplementary section (the final page) of the paper.","title":"Why was the batch size set to 32 for most of the results?"},{"location":"papers/mlrc/#why-werent-more-hard-mining-methods-evaluated","text":"We did test one loss+miner combination (Multi-similarity loss + their mining method). But we mainly wanted to do a thorough evaluation of loss functions, because that is the subject of most recent metric learning papers.","title":"Why weren't more hard-mining methods evaluated?"},{"location":"papers/mlrc/#for-the-contrastive-loss-why-is-the-optimal-positive-margin-a-negative-value","text":"A negative value should be equivalent to a margin of 0, because the distance between positive pairs cannot be negative, and the margin does not contribute to the gradient. So allowing the hyperparameter optimization to explore negative margins was unnecesary, but by the time I realized this, it wasn't worth changing the optimization bounds.","title":"For the contrastive loss, why is the optimal positive margin a negative value?"}]}